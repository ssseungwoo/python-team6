import os
import json
from collections import Counter, defaultdict
from konlpy.tag import Mecab

# ê¸°ë³¸ ì„¤ì •
DATA_DIR = "/Users/sseung/Documents/study/python_class/project_root/data/use/crawling"
OUTPUT_PATH = "/Users/sseung/Documents/study/python_class/project_root/data/use/hot_keyword/hot_keywords_by_company.json"
BROADCASTERS = ["KBS", "SBS", "YTN"]
TOPICS = ["IT_ê³¼í•™", "ê²½ì œ", "ì •ì¹˜", "ìŠ¤í¬ì¸ ", "ì—°ì˜ˆ", "ê¸°íƒ€"]

# ë¶ˆìš©ì–´ ì •ì˜ (ì›í•˜ëŠ” ëŒ€ë¡œ ë” ì¶”ê°€ ê°€ëŠ¥)
stopwords = set([
    "ê²ƒ", "ìˆë‹¤", "ì—†ë‹¤", "ì…ë‹ˆë‹¤", "ëŒ€í•œ", "í•œë‹¤", "í•˜ë©°", "ìˆ˜", "í•©ë‹ˆë‹¤", "ê²ë‹ˆë‹¤", "ë˜ë‹¤",
    "ê¸°ì", "ì˜ìƒ", "ë‰´ìŠ¤", "ë³´ë„", "ì•µì»¤", "ë§í•˜ë‹¤", "í†µí•´", "ìœ„í•´", "ì—ì„œ", "ìœ¼ë¡œ", "ì´ë‹¤", "í•˜ëŠ”",
    "ê°™ë‹¤", "ê·¸ë¦¬ê³ ", "í•˜ì§€ë§Œ", "ê·¸ëŸ¬ë‚˜", "ê²ë‹ˆë‹¤"
])

mecab = Mecab()
result = {b: {t: [] for t in TOPICS} for b in BROADCASTERS}

for broadcaster in BROADCASTERS:
    file_path = os.path.join(DATA_DIR, f"{broadcaster}_crawling_with_summary.json")
    if not os.path.exists(file_path):
        print(f"âŒ {file_path} ì—†ìŒ. ê±´ë„ˆëœë‹ˆë‹¤.")
        continue

    try:
        with open(file_path, "r", encoding="utf-8") as f:
            articles = json.load(f)
    except Exception as e:
        print(f"âš ï¸ {broadcaster} íŒŒì¼ ë¡œë”© ì¤‘ ì˜¤ë¥˜: {e}")
        continue

    topic_word_counts = defaultdict(Counter)

    for article in articles:
        try:
            topic = article.get("topic", "ê¸°íƒ€")
            if topic not in TOPICS:
                topic = "ê¸°íƒ€"

            text = article.get("cleaned_description", "") + " " + article.get("cleaned_title", "")
            if not text.strip():
                continue

            nouns = [
                word for word, tag in mecab.pos(text)
                if tag.startswith("NN") and word not in stopwords and len(word) > 1
            ]
            topic_word_counts[topic].update(nouns)

        except Exception as e:
            print(f"ğŸ›‘ ê¸°ì‚¬ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ID: {article.get('id', 'N/A')}): {e}")
            continue

    for topic in TOPICS:
        result[broadcaster][topic] = [
            word for word, _ in topic_word_counts[topic].most_common(10)
        ]

# ì €ì¥
try:
    with open(OUTPUT_PATH, "w", encoding="utf-8") as f:
        json.dump(result, f, indent=2, ensure_ascii=False)
    print(f"âœ… í•« í‚¤ì›Œë“œ ì €ì¥ ì™„ë£Œ: {OUTPUT_PATH}")
except Exception as e:
    print(f"âŒ ì €ì¥ ì‹¤íŒ¨: {e}")
