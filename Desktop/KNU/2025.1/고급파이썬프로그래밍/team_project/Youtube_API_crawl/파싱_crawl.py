import asyncio
import json
import re
from datetime import datetime, timezone, timedelta

import aiohttp
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
import pandas as pd
from dateutil import parser
from konlpy.tag import Okt # í˜•íƒœì†Œ ë¶„ì„ê¸°

# --- ì‚¬ìš©ì ì œê³µ ì „ì²˜ë¦¬ í•¨ìˆ˜ë“¤ ---
okt = Okt() # Okt í˜•íƒœì†Œ ë¶„ì„ê¸° ì¸ìŠ¤í„´ìŠ¤ (ì „ì—­ ë³€ìˆ˜ë¡œ í•œ ë²ˆë§Œ ìƒì„±)

def clean_title(title):
    if not isinstance(title, str):
        return ""
    title = re.sub(r"\s*/\s*(KBS|SBS|YTN).*", "", title)
    title = re.sub(r"\([^)]*\)", "", title)
    title = re.sub(r"\[[^\]]*\]", "", title)
    title = re.sub(r"\{[^}]*\}", "", title)
    title = re.sub(r"ã€[^ã€‘]*ã€‘", "", title)
    title = re.sub(r'([ê°€-í£ã…-ã…£])\1{1,}', '', title)
    title = re.sub(r'[ã…‹ã…ã… ã…œ]{2,}', '', title)
    title = re.sub(r'ì•—|í—‰|ìœ½|í¥|í’‰|ì—êµ¬|ì|ìœ¼ìŒ|ì•„ì•…|ë¼ì•¼|í‘¸í•˜í•˜|í•˜í•˜í•˜|íˆíˆíˆ|í—¤í—¤í—¤|ííí|ë‚„ë‚„|ê¹”ê¹”|ì½œë¡ì½œë¡|í›Œì©|ì‰¿', '', title)
    title = re.sub(r"[^\w\sê°€-í£.%]", " ", title)
    title = re.sub(r"[Â·â€¢]{3,}|\.{3,}|â€¦", " ", title)
    title = re.sub(r"\s+", " ", title)
    return title.strip()

def clean_text_full(text):
    if not isinstance(text, str):
        return ""
    text = re.sub(r'^(â–£|â—‡|â– |â–²|â–¼|â—†|â—‹|â—|â–³|â–·|â–¶|â€»|â—)\s*', ' ', text)
    text = re.sub(r'\([^()]*\)', ' ', text)
    text = re.sub(r'\{[^{}]*\}', ' ', text)
    text = re.sub(r'\[[^\[\]]*\]', ' ', text)
    lines = text.splitlines()
    lines = [line for line in lines if not line.strip().startswith(('â–£', 'â—‡', 'â– ', 'â–²', 'â–¼', 'â—†', 'â—‹', 'â—', 'â–³', 'â–·', 'â–¶', 'â€»', 'â—'))]
    text = ' '.join(lines)
    text = re.sub(r'([ê°€-í£]{2,4})(?:[ì´|ì„|ì„­|ë¦½|ìŒ|í•©|ìŠµ|ë]?)\bë‹ˆë‹¤', ' ', text)
    text = re.sub(r'([ê°€-í£]{2,4})(?:[ì´|ì„|ì„­|ë¦½|ìŒ|í•©|ìŠµ|ë]?)\bìŠµë‹ˆë‹¤', ' ', text)
    job_titles = 'ê¸°ì|ì•µì»¤|íŠ¹íŒŒì›|ê¸°ìƒìºìŠ¤í„°|ì§„í–‰|ì´¬ì˜ê¸°ì|ê·¸ë˜í”½|ë¦¬í¬í„°|ë…¼ì„¤ìœ„ì›|ë…¼í‰|í•´ì„¤ìœ„ì›|ì·¨ì¬|í˜„ì¥ê¸°ì|ë‰´ìŠ¤íŒ€|ë³´ë„êµ­|ì˜ìƒí¸ì§‘'
    text = re.sub(rf'([ê°€-í£]{{2,4}}\s*(?:{job_titles})(?:\s*[:/]\s*[ê°€-í£]{{2,4}}\s*(?:{job_titles})?)*)', ' ', text)
    text = re.sub(rf'((?:{job_titles})\s*[:/]\s*[ê°€-í£]{{2,4}}(?:\s*[:/]\s*(?:{job_titles})?\s*[ê°€-í£]{{2,4}})*)', ' ', text)
    endings = 'ì…ë‹ˆë‹¤|ì…ë‹ˆë‹¤\\.|ì…ë‹ˆë‹¤\\?|ì´ë‹¤|ì…ë‹ˆë‹¤ë§Œ|ì…ë‹ˆë‹¤ë§Œ\\.|ì´ë¼ê³ |ì´ë¼ëŠ”|ë¼ê³ |ëŠ”|ì€|ê°€|ì´ì—ˆìŠµë‹ˆë‹¤\\.|ì´ì—ˆìŠµë‹ˆë‹¤|ì˜€ìŠµë‹ˆë‹¤\\.|ì˜€ìŠµë‹ˆë‹¤'
    text = re.sub(rf'((?:{job_titles})\s*[:/]\s*[ê°€-í£]{{2,4}}(?:\s*[:/]\s*(?:{job_titles})?\s*[ê°€-í£]{{2,4}})*)', ' ', text)
    text = re.sub(r'\'[ê°€-í£\s]+\'\s*[ê°€-í£]{2,10}(?:[ ]*ì˜€ìŠµë‹ˆë‹¤\\.|ì˜€ìŠµë‹ˆë‹¤|ì´ì—ˆìŠµë‹ˆë‹¤\\.|ì´ì—ˆìŠµë‹ˆë‹¤)\s*', ' ', text)
    text = re.sub(r'[ê°€-í£]{2,4}ì˜\s+[ê°€-í£\s]+(?:[ ]*ì˜€ìŠµë‹ˆë‹¤\\.|ì˜€ìŠµë‹ˆë‹¤|ì´ì—ˆìŠµë‹ˆë‹¤\\.|ì´ì—ˆìŠµë‹ˆë‹¤)\s*', ' ', text)
    text = re.sub(rf'([ê°€-í£]{{2,4}}\s*(?:{job_titles})\s*(?:{endings})\s*)', ' ', text)
    text = re.sub(rf'([ê°€-í£]{{2,4}}\s*(?:{job_titles})\s*)', ' ', text)
    text = re.sub(r'\b(ê¸°ì|ì•µì»¤|íŠ¹íŒŒì›|ê¸°ìƒìºìŠ¤í„°|ì§„í–‰|ì´¬ì˜ê¸°ì|ê·¸ë˜í”½|ë¦¬í¬í„°|ë…¼ì„¤ìœ„ì›|ë…¼í‰|í•´ì„¤ìœ„ì›|ì·¨ì¬|í˜„ì¥ê¸°ì|ë‰´ìŠ¤íŒ€|ë³´ë„êµ­)\b', ' ', text)
    text = re.sub(r'(ë‰´ìŠ¤\s?[ê°€-í£]{1,10}ì…ë‹ˆë‹¤[.]?)|(ê¸°ìì…ë‹ˆë‹¤[.]?)|(ê¸°ì¡ë‹ˆë‹¤[.]?)|(ë³´ë„í•©ë‹ˆë‹¤[.]?)|(ì „í•©ë‹ˆë‹¤[.]?)|(ì „í•´ë“œë¦½ë‹ˆë‹¤[.]?)', ' ', text)
    text = re.sub(r'[ê°€-í£]{2,10}\s*:\s*[ê°€-í£\s]{2,100}', ' ', text)
    text = re.sub(r'[ê°€-í£]{2,7}\s*:\s*[ê°€-í£]{2,6}(?:[ /Â·ã†,][ê°€-í£]{2,6})*', ' ', text)
    text = re.sub(r'["â€œâ€â€˜â€™\'`]', ' ', text)
    text = re.sub(r'(KBS|SBS|YTN)\së‰´ìŠ¤\s[ê°€-í£]{1,4}((ì…|ì´|í•©|ìŠµ|ë)?ì…ë‹ˆë‹¤)', ' ', text)
    text = re.sub(r'\b(KBS|SBS|YTN)\b', ' ', text)
    text = re.sub(r'\b\d{1,3}(?:,\d{3})*(?:\.\d+)?[ê°€-í£a-zA-Z%â„ƒ]+\b', ' ', text)
    text = re.sub(r'\b\d{1,3}(?:,\d{3})*(?:\.\d+)?\b', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text.strip()

def clean_text_for_vectorization(text):
    if not isinstance(text, str):
        return ""
    cleaned_initial_text = clean_text_full(text)
    cleaned_initial_text = re.sub(r'([ê°€-í£ã…-ã…£])\1{1,}', '', cleaned_initial_text)
    cleaned_initial_text = re.sub(r'[ã…‹ã…ã… ã…œ]{2,}', '', cleaned_initial_text)
    cleaned_initial_text = re.sub(r'[^\w\sê°€-í£.]', ' ', cleaned_initial_text)
    cleaned_initial_text = re.sub(r'\s+', ' ', cleaned_initial_text).strip()
    words = []
    for word, tag in okt.pos(cleaned_initial_text, norm=True, stem=True):
        if tag in ['Noun', 'Verb', 'Adjective', 'Adverb']:
            words.append(word)
    korean_stopwords = [
        'ì˜¤ëŠ˜', 'ì´ë²ˆ', 'ì§€ë‚œ', 'ë˜ë‹¤', 'í•˜ë‹¤', 'ìˆë‹¤', 'ì´ë‹¤', 'ê²ƒ', 'ìˆ˜', 'ê·¸', 'ë”', 'ì¢€', 'ì˜',
        'ê°€ì¥', 'ë‹¤', 'ë˜', 'ë§ì´', 'ê·¸ë¦¬ê³ ', 'ê·¸ëŸ¬ë‚˜', 'í•˜ì§€ë§Œ', 'ë”°ë¼', 'ë“±', 'ë“±ë“±', 'í†µí•´',
        'ê¹Œì§€', 'ë¶€í„°', 'ëŒ€í•œ', 'ìœ¼ë¡œ', 'ì—ì„œ', 'ì—ê²Œ', 'ì—ê²Œì„œ', 'ë³´ë‹¤', 'ë•Œë¬¸',
        'ìŠµë‹ˆë‹¤', 'í•©ë‹ˆë‹¤', 'í•©ë‹ˆë‹¤ë§Œ', 'ì…ë‹ˆë‹¤ë§Œ', 'ì´ë¼ê³ ', 'ì´ì—ˆìŠµë‹ˆë‹¤', 'ì˜€ìŠµë‹ˆë‹¤',
        'ë…„', 'ì›”', 'ì¼', 'ì‹œ', 'ë¶„', 'ì´ˆ', 'ì˜¤ì „', 'ì˜¤í›„', 'ì´ë²ˆì£¼', 'ì§€ë‚œì£¼', 'ë‹¤ìŒì£¼', 'ì´ë‹¬', 'ì§€ë‚œë‹¬', 'ë‹¤ìŒë‹¬', 'ì˜¬í•´', 'ì§€ë‚œí•´', 'ë‚´ë…„'
    ]
    words = [word for word in words if word not in korean_stopwords and len(word) > 1]
    return ' '.join(words)

# --- ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ (ê¸°ì¡´ê³¼ ë™ì¼) ---

def parse_duration_to_seconds(duration_str):
    """
    ISO 8601 ê¸°ê°„ ë¬¸ìì—´ (ì˜ˆ: PT96S, PT1M30S)ì„ ì´ˆ ë‹¨ìœ„ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    Seleniumìœ¼ë¡œ ë°›ì€ "Xë¶„ Yì´ˆ" í˜•íƒœë„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    if isinstance(duration_str, int): # ì´ë¯¸ ì´ˆ ë‹¨ìœ„ ì •ìˆ˜ì¼ ê²½ìš°
        return duration_str
    
    duration_str = str(duration_str).upper()

    # ISO 8601 Duration (e.g., PT96S, PT1M30S)
    iso_match = re.match(r'PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?', duration_str)
    if iso_match:
        hours, minutes, seconds = iso_match.groups()
        total_seconds = 0
        if hours:
            total_seconds += int(hours) * 3600
        if minutes:
            total_seconds += int(minutes) * 60
        if seconds:
            total_seconds += int(seconds)
        return total_seconds
    
    # "Xë¶„ Yì´ˆ" (e.g., "1ë¶„ 30ì´ˆ", "20ì´ˆ")
    korean_match = re.match(r'(?:(\d+)ë¶„\s*)?(?:(\d+)ì´ˆ)?', duration_str)
    if korean_match:
        minutes_korean, seconds_korean = korean_match.groups()
        total_seconds = 0
        if minutes_korean:
            total_seconds += int(minutes_korean) * 60
        if seconds_korean:
            total_seconds += int(seconds_korean)
        return total_seconds

    return 0 # íŒŒì‹± ì‹¤íŒ¨ ì‹œ 0 ë°˜í™˜

def convert_upload_date_to_kst(iso_datetime_str):
    """
    ISO 8601 í˜•ì‹ì˜ ë‚ ì§œ ë¬¸ìì—´ì„ KST (UTC+9)ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    try:
        dt_with_tz = parser.isoparse(iso_datetime_str)
        kst_timezone = timezone(timedelta(hours=9))
        dt_kst = dt_with_tz.astimezone(kst_timezone)
        return dt_kst.strftime("%Y-%m-%d %H:%M:%S")
    except Exception as e:
        print(f"â›” uploadDate ì‹œê°„ íŒŒì‹± ì‹¤íŒ¨: {e}")
        return ""

# --- YouTube í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤ ---

class YouTubeNewsCrawler:
    def __init__(self, channel_url, max_videos_to_collect=200, min_duration_sec=60, max_duration_sec=300, recent_hours=24):
        self.channel_url = channel_url
        self.max_videos_to_collect = max_videos_to_collect
        self.min_duration_sec = min_duration_sec
        self.max_duration_sec = max_duration_sec
        self.recent_hours = recent_hours
        self.driver = None # Selenium WebDriver ì¸ìŠ¤í„´ìŠ¤
        self.collected_links = set() # ì¤‘ë³µ ë§í¬ ë°©ì§€
        self.results = [] # ìµœì¢… ê²°ê³¼ ì €ì¥ (1ì°¨ í•„í„°ë§ í›„)
        
        # í¬ë¡¤ë§ ì‹œì‘ ì‹œê°„ì„ KSTë¡œ ê¸°ë¡ (24ì‹œê°„ í•„í„°ë§ ê¸°ì¤€)
        self.crawl_start_time_kst = datetime.now(timezone(timedelta(hours=9)))

    def _init_driver(self):
        """Selenium WebDriverë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        print("ğŸŒ WebDriver ì´ˆê¸°í™” ì¤‘...")
        options = Options()
        options.add_argument("--headless")  # GUI ì—†ì´ ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-gpu")
        options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")
        options.add_argument("lang=ko_KR") # í•œêµ­ì–´ ì„¤ì •
        
        service = Service(ChromeDriverManager().install())
        self.driver = webdriver.Chrome(service=service, options=options)
        print("âœ… WebDriver ì´ˆê¸°í™” ì™„ë£Œ.")

    def _close_driver(self):
        """Selenium WebDriverë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤."""
        if self.driver:
            print("ğŸ‘‹ WebDriver ì¢…ë£Œ ì¤‘...")
            self.driver.quit()
            self.driver = None
            print("âœ… WebDriver ì¢…ë£Œ ì™„ë£Œ.")

    async def _scroll_to_end(self):
        """í˜ì´ì§€ë¥¼ ëê¹Œì§€ ìŠ¤í¬ë¡¤í•˜ì—¬ ë™ì  ì½˜í…ì¸ ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        last_height = self.driver.execute_script("return document.documentElement.scrollHeight")
        scroll_attempts = 0
        max_scroll_attempts = 100 # ë¬´í•œ ìŠ¤í¬ë¡¤ ë°©ì§€ë¥¼ ìœ„í•œ ìµœëŒ€ ìŠ¤í¬ë¡¤ ì‹œë„ íšŸìˆ˜
        
        while True:
            self.driver.execute_script("window.scrollTo(0, document.documentElement.scrollHeight);")
            await asyncio.sleep(2) # í˜ì´ì§€ ë¡œë“œë¥¼ ìœ„í•´ ì ì‹œ ëŒ€ê¸°
            
            new_height = self.driver.execute_script("return document.documentElement.scrollHeight")
            if new_height == last_height:
                scroll_attempts += 1
                if scroll_attempts > 3: # 3ë²ˆ ìŠ¤í¬ë¡¤í•´ë„ ìƒˆ ì½˜í…ì¸  ì—†ìœ¼ë©´ ì¢…ë£Œ
                    break
            else:
                scroll_attempts = 0 # ìƒˆ ì½˜í…ì¸  ë¡œë“œ ì‹œ ì‹œë„ íšŸìˆ˜ ì´ˆê¸°í™”
            last_height = new_height
            
            # ìˆ˜ì§‘ ëª©í‘œ ì˜ìƒ ê°œìˆ˜ ì´ìƒì´ ë¡œë“œë˜ë©´ ìŠ¤í¬ë¡¤ ì¤‘ë‹¨
            if len(self.driver.find_elements(By.CSS_SELECTOR, "ytd-rich-item-renderer")) >= self.max_videos_to_collect:
                print(f"ğŸŒŸ {self.max_videos_to_collect}ê°œ ì´ìƒì˜ ì˜ìƒì´ ë¡œë“œë˜ì–´ ìŠ¤í¬ë¡¤ ì¤‘ë‹¨.")
                break
            
            if scroll_attempts >= max_scroll_attempts:
                print(f"âš ï¸ ìµœëŒ€ ìŠ¤í¬ë¡¤ ì‹œë„ íšŸìˆ˜({max_scroll_attempts}) ë„ë‹¬, ìŠ¤í¬ë¡¤ ì¤‘ë‹¨.")
                break


    async def collect_video_links_and_filter_length(self):
        """
        Seleniumìœ¼ë¡œ ì±„ë„ í˜ì´ì§€ë¥¼ ìŠ¤í¬ë¡¤í•˜ë©° ì˜ìƒ ë§í¬ì™€ ì œëª©ì„ ìˆ˜ì§‘í•˜ê³ ,
        ì˜ìƒ ê¸¸ì´(1~5ë¶„)ë¥¼ 1ì°¨ í•„í„°ë§í•©ë‹ˆë‹¤.
        """
        print(f"ğŸ¬ ì±„ë„ ë™ì˜ìƒ íƒ­ ì ‘ì†: {self.channel_url}")
        
        await asyncio.to_thread(self._init_driver) # WebDriver ì´ˆê¸°í™”ëŠ” ë™ê¸° í•¨ìˆ˜ì´ë¯€ë¡œ to_thread ì‚¬ìš©
        
        try:
            await asyncio.to_thread(self.driver.get, self.channel_url)
            
            WebDriverWait(self.driver, 20).until(
                EC.presence_of_element_located((By.ID, "contents"))
            )
            print("âœ… ì±„ë„ í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ. ì˜ìƒ ëª©ë¡ ìˆ˜ì§‘ ì‹œì‘...")

            await self._scroll_to_end() # ë¹„ë™ê¸° ìŠ¤í¬ë¡¤

            video_elements = await asyncio.to_thread(self.driver.find_elements, By.CSS_SELECTOR, "ytd-rich-item-renderer")
            print(f"ğŸ” ì´ {len(video_elements)}ê°œì˜ ì˜ìƒ ìš”ì†Œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")

            for i, element in enumerate(video_elements):
                if len(self.results) >= self.max_videos_to_collect:
                    print(f"ğŸ”¥ ëª©í‘œ ì˜ìƒ ê°œìˆ˜({self.max_videos_to_collect}) ë„ë‹¬, ìˆ˜ì§‘ ì¤‘ë‹¨.")
                    break

                try:
                    link_element = element.find_element(By.CSS_SELECTOR, "a#video-title-link")
                    video_link = link_element.get_attribute("href")
                    if not video_link: continue

                    if video_link.startswith("/watch"):
                        video_link = f"https://www.youtube.com{video_link}"
                    
                    if video_link in self.collected_links: continue

                    title = link_element.get_attribute("title") or link_element.text
                    if not title: continue

                    # ì˜ìƒ ê¸¸ì´ ì¶”ì¶œ ë° 1ì°¨ í•„í„°ë§
                    duration_text = None
                    try:
                        # ì¸ë„¤ì¼ ìœ„ì— í‘œì‹œë˜ëŠ” ì‹œê°„ ì •ë³´ ì¶”ì¶œ
                        duration_element = element.find_element(By.CSS_SELECTOR, "ytd-thumbnail-overlay-time-status-renderer #text")
                        duration_text = duration_element.text.strip()
                    except:
                        pass 

                    video_duration_sec = parse_duration_to_seconds(duration_text)

                    if not (self.min_duration_sec <= video_duration_sec < self.max_duration_sec):
                        # print(f"â³ ì˜ìƒ ê¸¸ì´ í•„í„°ë§: '{title}' ({duration_text}, {video_duration_sec}ì´ˆ). ì¡°ê±´ ë¶ˆì¼ì¹˜.")
                        continue 

                    self.collected_links.add(video_link)
                    
                    # 1ì°¨ í•„í„°ë§ í†µê³¼í•œ ì •ë³´ ì €ì¥ (ë‚˜ì¤‘ì— ìƒì„¸ ì •ë³´ ì±„ì›€)
                    self.results.append({
                        'title': title,
                        'link': video_link
                    })
                    print(f"âœ… 1ì°¨ í•„í„°ë§ í†µê³¼: '{title}' ({duration_text})")

                except Exception as e:
                    print(f"âš ï¸ ì˜ìƒ ìš”ì†Œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e} - ìš”ì†Œ {i+1}")
                    continue
            
            print(f"âœ¨ 1ì°¨ í•„í„°ë§ í›„ ìˆ˜ì§‘ëœ ì˜ìƒ ë§í¬ ê°œìˆ˜: {len(self.results)}ê°œ")

        finally:
            await asyncio.to_thread(self._close_driver) # ë“œë¼ì´ë²„ ì¢…ë£Œ

    async def get_video_details_from_link(self, video_info):
        """
        ê°œë³„ ì˜ìƒ í˜ì´ì§€ì— ì ‘ì†í•˜ì—¬ JSON-LDì—ì„œ ìƒì„¸ ì •ë³´(í”„ë¡¬í”„íŠ¸, ì—…ë¡œë“œ ì‹œê°„ ë“±)ë¥¼ ì¶”ì¶œí•˜ê³ ,
        ì—…ë¡œë“œ ì‹œê°„(24ì‹œê°„ ì´ë‚´)ì„ 2ì°¨ í•„í„°ë§í•©ë‹ˆë‹¤.
        """
        video_link = video_info['link']
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(video_link) as response:
                    response.raise_for_status() # HTTP ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ˆì™¸ ë°œìƒ
                    html_content = await response.text()

            soup = BeautifulSoup(html_content, 'html.parser')
            
            # JSON-LD ë°ì´í„° ì¶”ì¶œ
            json_ld_script = soup.find('script', type='application/ld+json')
            
            if not json_ld_script:
                # print(f"âŒ JSON-LD ìŠ¤í¬ë¦½íŠ¸ ì—†ìŒ: {video_link}")
                return None 

            json_data = json.loads(json_ld_script.string)

            # --- JSON-LDì—ì„œ ì •ë³´ ì¶”ì¶œ (ì‚¬ìš©ì ìš”ì²­ í•„ë“œë§Œ) ---
            
            # 1. ì—…ë¡œë“œ ì‹œê°„ (uploadDate) - 2ì°¨ í•„í„°ë§ ê¸°ì¤€
            upload_date_iso = json_data.get('uploadDate')
            if not upload_date_iso:
                # print(f"âŒ uploadDate ì—†ìŒ: {video_link}")
                return None

            upload_dt_kst = parser.isoparse(upload_date_iso).astimezone(timezone(timedelta(hours=9)))
            
            # 2ì°¨ í•„í„°ë§: 24ì‹œê°„ ì´ë‚´ ì¡°ê±´
            time_difference = self.crawl_start_time_kst - upload_dt_kst
            if time_difference.total_seconds() > (self.recent_hours * 3600):
                # print(f"â° ì—…ë¡œë“œ ì‹œê°„ í•„í„°ë§: '{video_info['title']}' ({upload_dt_kst.strftime('%Y-%m-%d %H:%M:%S KST')}). 24ì‹œê°„ ì´ˆê³¼.")
                return None 

            # 2. ì˜ìƒ ì„¤ëª… (í”„ë¡¬í”„íŠ¸) - JSON-LDì—ì„œ ì§ì ‘ ê°€ì ¸ì˜´
            raw_prompt = json_data.get('description', '')

            # 3. ì¸ë„¤ì¼ URL
            thumbnail_urls = json_data.get('thumbnailUrl', [])
            thumbnail_link = thumbnail_urls[0] if thumbnail_urls else ''
            
            # --- ë°ì´í„° ì „ì²˜ë¦¬ ---
            cleaned_title = clean_title(video_info['title'])
            cleaned_prompt = clean_text_for_vectorization(raw_prompt)
            
            # ìµœì¢… ê²°ê³¼ ë°˜í™˜ (ìš”ì²­í•˜ì‹  í•„ë“œë§Œ í¬í•¨)
            return {
                'id': None, # IDëŠ” ë‚˜ì¤‘ì— DataFrameì—ì„œ ë¶€ì—¬
                'raw_title': video_info['title'],
                'cleaned_title': cleaned_title,
                'raw_prompt': raw_prompt,
                'cleaned_prompt': cleaned_prompt,
                'link': video_link,
                'upload_date_kst': upload_dt_kst.strftime("%Y-%m-%d %H:%M:%S"),
                'thumbnail_link': thumbnail_link
            }

        except aiohttp.ClientError as e:
            print(f"âŒ HTTP ìš”ì²­ ì˜¤ë¥˜ ({video_link}): {e}")
        except json.JSONDecodeError as e:
            print(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜ ({video_link}): {e}")
        except Exception as e:
            print(f"âš ï¸ ê°œë³„ ì˜ìƒ ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({video_link}): {e}")
        return None

    async def run_crawler(self):
        """í¬ë¡¤ëŸ¬ì˜ ì „ì²´ ì‹¤í–‰ íë¦„ì„ ê´€ë¦¬í•©ë‹ˆë‹¤."""
        print(f"ğŸš€ í¬ë¡¤ë§ ì‹œì‘: {self.crawl_start_time_kst.strftime('%Y-%m-%d %H:%M:%S KST')} ê¸°ì¤€ {self.recent_hours}ì‹œê°„ ì´ë‚´ ì˜ìƒ ìˆ˜ì§‘")
        
        start_time = datetime.now()

        # 1ë‹¨ê³„: Seleniumìœ¼ë¡œ ë§í¬ ìˆ˜ì§‘ ë° 1ì°¨ ê¸¸ì´ í•„í„°ë§
        await self.collect_video_links_and_filter_length()

        if not self.results:
            print("ğŸš« 1ì°¨ í•„í„°ë§ í›„ ìˆ˜ì§‘ëœ ì˜ìƒì´ ì—†ìŠµë‹ˆë‹¤. í¬ë¡¤ë§ ì¢…ë£Œ.")
            return pd.DataFrame()

        print(f"\nâš¡ 2ë‹¨ê³„: ìˆ˜ì§‘ëœ {len(self.results)}ê°œ ì˜ìƒì˜ ìƒì„¸ ì •ë³´ ì¶”ì¶œ ë° 2ì°¨ ì‹œê°„ í•„í„°ë§ ì‹œì‘...")
        
        final_video_data = []
        for i, video_info in enumerate(self.results):
            print(f"ì²˜ë¦¬ ì¤‘... ({i+1}/{len(self.results)}) - {video_info['title']}")
            details = await self.get_video_details_from_link(video_info)
            if details:
                final_video_data.append(details)

        if not final_video_data:
            print("ğŸš« ëª¨ë“  í•„í„°ë§ì„ í†µê³¼í•œ ì˜ìƒì´ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()

        # Pandas DataFrameìœ¼ë¡œ ë³€í™˜
        df = pd.DataFrame(final_video_data)
        df['id'] = range(1, len(df) + 1) # ìˆœì°¨ì  ID ë¶€ì—¬
        
        end_time = datetime.now()
        total_time = end_time - start_time
        
        print(f"\nğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ! ì´ {len(df)}ê°œì˜ ì˜ìƒ ì •ë³´ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
        print(f"ì´ ì†Œìš” ì‹œê°„: {total_time}")
        
        # ê²°ê³¼ ì €ì¥ (ì„ íƒ ì‚¬í•­)
        output_filename = "youtube_news_videos_crawled.json"
        df.to_json(output_filename, orient="records", indent=4, force_ascii=False)
        print(f"ê²°ê³¼ê°€ '{output_filename}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return df

# --- ë©”ì¸ ì‹¤í–‰ ---

async def main():
    # ì‹¤ì œ í¬ë¡¤ë§í•  YouTube ì±„ë„ì˜ ë™ì˜ìƒ íƒ­ URLì„ ì—¬ê¸°ì— ì…ë ¥í•˜ì„¸ìš”.
    # ì´ì „ì— 'https://www.youtube.com/@newskbs/videos' ë¡œ ì£¼ì…¨ìœ¼ë‚˜, ì´ëŠ” ì˜ˆì‹œì´ë©° ì‹¤ì œ ì‘ë™í•˜ëŠ” URLë¡œ ë³€ê²½í•´ì•¼ í•©ë‹ˆë‹¤.
    target_channel_url = "https://www.youtube.com/@newskbs/videos" 
    
    # í¬ë¡¤ë§ ì„¤ì • (íŒŒë¼ë¯¸í„° ì¡°ì • ê°€ëŠ¥)
    crawler = YouTubeNewsCrawler(
        channel_url=target_channel_url,
        max_videos_to_collect=50,  # 1ì°¨ í•„í„°ë§ ì „ ìµœëŒ€ ìˆ˜ì§‘ ì‹œë„í•  ì˜ìƒ ê°œìˆ˜ (í…ŒìŠ¤íŠ¸ ì‹œ ì ê²Œ ì„¤ì •)
        min_duration_sec=60,      # ìµœì†Œ ì˜ìƒ ê¸¸ì´ (1ë¶„)
        max_duration_sec=300,     # ìµœëŒ€ ì˜ìƒ ê¸¸ì´ (5ë¶„)
        recent_hours=24           # ìµœê·¼ ì—…ë¡œë“œ ì‹œê°„ (24ì‹œê°„ ì´ë‚´)
    )
    
    crawled_df = await crawler.run_crawler()
    
    if not crawled_df.empty:
        print("\n--- ìˆ˜ì§‘ëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° ---")
        print(crawled_df[['id', 'cleaned_title', 'upload_date_kst', 'duration_sec', 'link']].head())
        print(f"\nìµœì¢… DataFrame í¬ê¸°: {len(crawled_df)}ê°œ ì˜ìƒ")
    else:
        print("ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    asyncio.run(main())