import time
import re
import pandas as pd
import json
import asyncio # ë¹„ë™ê¸° ì²˜ë¦¬ë¥¼ ìœ„í•œ ëª¨ë“ˆ
import aiohttp # ë¹„ë™ê¸° HTTP ìš”ì²­ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ (ì„ íƒ ì‚¬í•­, ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ì— í™œìš©)
from bs4 import BeautifulSoup # HTML íŒŒì‹±ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ (ì„ íƒ ì‚¬í•­, ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ì— í™œìš©)

from datetime import datetime, timezone, timedelta
from dateutil import parser

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException

from webdriver_manager.chrome import ChromeDriverManager

from konlpy.tag import Okt

# Okt ê°ì²´ëŠ” í•œ ë²ˆë§Œ ìƒì„±í•˜ì—¬ ì¬ì‚¬ìš©í•˜ëŠ” ê²ƒì´ íš¨ìœ¨ì ì…ë‹ˆë‹¤.
okt = Okt()

# --- ê¸°ì¡´ ì „ì²˜ë¦¬ í•¨ìˆ˜ë“¤ (ë™ì¼í•˜ê²Œ ì‚¬ìš©) ---
def clean_title(title):
    if not isinstance(title, str):
        return ""
    title = re.sub(r"\s*/\s*(KBS|SBS|YTN).*", "", title)
    title = re.sub(r"\([^)]*\)", "", title)
    title = re.sub(r"\[[^\]]*\]", "", title)
    title = re.sub(r"\{[^}]*\}", "", title)
    title = re.sub(r"ã€[^ã€‘]*ã€‘", "", title)
    title = re.sub(r'([ê°€-í£ã…-ã…£])\1{1,}', '', title)
    title = re.sub(r'[ã…‹ã…ã… ã…œ]{2,}', '', title)
    title = re.sub(r'ì•—|í—‰|ìœ½|í¥|í’‰|ì—êµ¬|ì|ìœ¼ìŒ|ì•„ì•…|ë¼ì•¼|í‘¸í•˜í•˜|í•˜í•˜í•˜|íˆíˆíˆ|í—¤í—¤í—¤|ííí|ë‚„ë‚„|ê¹”ê¹”|ì½œë¡ì½œë¡|í›Œì©|ì‰¿', '', title)
    title = re.sub(r"[^\w\sê°€-í£.%]", " ", title)
    title = re.sub(r"[Â·â€¢]{3,}|\.{3,}|â€¦", " ", title)
    title = re.sub(r"\s+", " ", title)
    return title.strip()

def clean_text_full(text, apply_morphs=False):
    if not isinstance(text, str):
        return ""
    text = re.sub(r'^(â–£|â—‡|â– |â–²|â–¼|â—†|â—‹|â—|â–³|â–·|â–¶|â€»|â—)\s*', ' ', text)
    text = re.sub(r'\([^()]*\)', ' ', text)
    text = re.sub(r'\{[^{}]*\}', ' ', text)
    text = re.sub(r'\[[^\[\]]*\]', ' ', text)
    lines = text.splitlines()
    lines = [line for line in lines if not line.strip().startswith(('â–£', 'â—‡', 'â– ', 'â–²', 'â–¼', 'â—†', 'â—‹', 'â—', 'â–³', 'â–·', 'â–¶', 'â€»', 'â—'))]
    text = ' '.join(lines)
    text = re.sub(r'([ê°€-í£]{2,4})(?:[ì´|ì„|ì„­|ë¦½|ìŒ|í•©|ìŠµ|ë]?)\bë‹ˆë‹¤', ' ', text)
    text = re.sub(r'([ê°€-í£]{2,4})(?:[ì´|ì„|ì„­|ë¦½|ìŒ|í•©|ìŠµ|ë]?)\bìŠµë‹ˆë‹¤', ' ', text)
    job_titles = 'ê¸°ì|ì•µì»¤|íŠ¹íŒŒì›|ê¸°ìƒìºìŠ¤í„°|ì§„í–‰|ì´¬ì˜ê¸°ì|ê·¸ë˜í”½|ë¦¬í¬í„°|ë…¼ì„¤ìœ„ì›|ë…¼í‰|í•´ì„¤ìœ„ì›|ì·¨ì¬|í˜„ì¥ê¸°ì|ë‰´ìŠ¤íŒ€|ë³´ë„êµ­|ì˜ìƒí¸ì§‘'
    text = re.sub(rf'([ê°€-í£]{{2,4}}\s*(?:{job_titles})(?:\s*[:/]\s*[ê°€-í£]{{2,4}}\s*(?:{job_titles})?)*)', ' ', text)
    text = re.sub(rf'((?:{job_titles})\s*[:/]\s*[ê°€-í£]{{2,4}}(?:\s*[:/]\s*(?:{job_titles})?\s*[ê°€-í£]{{2,4}})*)', ' ', text)
    endings = 'ì…ë‹ˆë‹¤|ì…ë‹ˆë‹¤\\.|ì…ë‹ˆë‹¤\\?|ì´ë‹¤|ì…ë‹ˆë‹¤ë§Œ|ì…ë‹ˆë‹¤ë§Œ\\.|ì´ë¼ê³ |ì´ë¼ëŠ”|ë¼ê³ |ëŠ”|ì€|ê°€|ì´ì—ˆìŠµë‹ˆë‹¤\\.|ì´ì—ˆìŠµë‹ˆë‹¤|ì˜€ìŠµë‹ˆë‹¤\\.|ì˜€ìŠµë‹ˆë‹¤'
    text = re.sub(rf'((?:{job_titles})\s*[:/]\s*[ê°€-í£]{{2,4}}(?:\s*[:/]\s*(?:{job_titles})?\s*[ê°€-í£]{{2,4}})*)', ' ', text)
    text = re.sub(r'\'[ê°€-í£\s]+\'\s*[ê°€-í£]{2,10}(?:[ ]*ì˜€ìŠµë‹ˆë‹¤\\.|ì˜€ìŠµë‹ˆë‹¤|ì´ì—ˆìŠµë‹ˆë‹¤\\.|ì´ì—ˆìŠµë‹ˆë‹¤)\s*', ' ', text)
    text = re.sub(r'[ê°€-í£]{2,4}ì˜\s+[ê°€-í£\s]+(?:[ ]*ì˜€ìŠµë‹ˆë‹¤\\.|ì˜€ìŠµë‹ˆë‹¤|ì´ì—ˆìŠµë‹ˆë‹¤\\.|ì´ì—ˆìŠµë‹ˆë‹¤)\s*', ' ', text)
    text = re.sub(rf'([ê°€-í£]{{2,4}}\s*(?:{job_titles})\s*(?:{endings})\s*)', ' ', text)
    text = re.sub(rf'([ê°€-í£]{{2,4}}\s*(?:{job_titles})\s*)', ' ', text)
    text = re.sub(r'\b(ê¸°ì|ì•µì»¤|íŠ¹íŒŒì›|ê¸°ìƒìºìŠ¤í„°|ì§„í–‰|ì´¬ì˜ê¸°ì|ê·¸ë˜í”½|ë¦¬í¬í„°|ë…¼ì„¤ìœ„ì›|ë…¼í‰|í•´ì„¤ìœ„ì›|ì·¨ì¬|í˜„ì¥ê¸°ì|ë‰´ìŠ¤íŒ€|ë³´ë„êµ­)\b', ' ', text)
    text = re.sub(r'(ë‰´ìŠ¤\s?[ê°€-í£]{1,10}ì…ë‹ˆë‹¤[.]?)|(ê¸°ìì…ë‹ˆë‹¤[.]?)|(ê¸°ì¡ë‹ˆë‹¤[.]?)|(ë³´ë„í•©ë‹ˆë‹¤[.]?)|(ì „í•©ë‹ˆë‹¤[.]?)|(ì „í•´ë“œë¦½ë‹ˆë‹¤[.]?)', ' ', text)
    text = re.sub(r'[ê°€-í£]{2,10}\s*:\s*[ê°€-í£\s]{2,100}', ' ', text)
    text = re.sub(r'[ê°€-í£]{2,7}\s*:\s*[ê°€-í£]{2,6}(?:[ /Â·ã†,][ê°€-í£]{2,6})*', ' ', text)
    text = re.sub(r'["â€œâ€â€˜â€™\'`]', ' ', text)
    text = re.sub(r'(KBS|SBS|YTN)\së‰´ìŠ¤\s[ê°€-í£]{1,4}((ì…|ì´|í•©|ìŠµ|ë)?ì…ë‹ˆë‹¤)', ' ', text)
    text = re.sub(r'\b(KBS|SBS|YTN)\b', ' ', text)
    text = re.sub(r'\b\d{1,3}(?:,\d{3})*(?:\.\d+)?[ê°€-í£a-zA-Z%â„ƒ]+\b', ' ', text)
    text = re.sub(r'\b\d{1,3}(?:,\d{3})*(?:\.\d+)?\b', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text.strip()

def clean_text_for_vectorization(text):
    if not isinstance(text, str):
        return ""
    cleaned_initial_text = clean_text_full(text)
    cleaned_initial_text = re.sub(r'([ê°€-í£ã…-ã…£])\1{1,}', '', cleaned_initial_text)
    cleaned_initial_text = re.sub(r'[ã…‹ã…ã… ã…œ]{2,}', '', cleaned_initial_text)
    cleaned_initial_text = re.sub(r'[^\w\sê°€-í£.]', ' ', cleaned_initial_text)
    cleaned_initial_text = re.sub(r'\s+', ' ', cleaned_initial_text).strip()
    words = []
    for word, tag in okt.pos(cleaned_initial_text, norm=True, stem=True):
        if tag in ['Noun', 'Verb', 'Adjective', 'Adverb']:
            words.append(word)
    korean_stopwords = [
        'ì˜¤ëŠ˜', 'ì´ë²ˆ', 'ì§€ë‚œ', 'ë˜ë‹¤', 'í•˜ë‹¤', 'ìˆë‹¤', 'ì´ë‹¤', 'ê²ƒ', 'ìˆ˜', 'ê·¸', 'ë”', 'ì¢€', 'ì˜',
        'ê°€ì¥', 'ë‹¤', 'ë˜', 'ë§ì´', 'ê·¸ë¦¬ê³ ', 'ê·¸ëŸ¬ë‚˜', 'í•˜ì§€ë§Œ', 'ë”°ë¼', 'ë“±', 'ë“±ë“±', 'í†µí•´',
        'ê¹Œì§€', 'ë¶€í„°', 'ëŒ€í•œ', 'ìœ¼ë¡œ', 'ì—ì„œ', 'ì—ê²Œ', 'ì—ê²Œì„œ', 'ë³´ë‹¤', 'ë•Œë¬¸',
        'ìŠµë‹ˆë‹¤', 'í•©ë‹ˆë‹¤', 'í•©ë‹ˆë‹¤ë§Œ', 'ì…ë‹ˆë‹¤ë§Œ', 'ì´ë¼ê³ ', 'ì´ì—ˆìŠµë‹ˆë‹¤', 'ì˜€ìŠµë‹ˆë‹¤',
        'ë…„', 'ì›”', 'ì¼', 'ì‹œ', 'ë¶„', 'ì´ˆ', 'ì˜¤ì „', 'ì˜¤í›„', 'ì´ë²ˆì£¼', 'ì§€ë‚œì£¼', 'ë‹¤ìŒì£¼', 'ì´ë‹¬', 'ì§€ë‚œë‹¬', 'ë‹¤ìŒë‹¬', 'ì˜¬í•´', 'ì§€ë‚œí•´', 'ë‚´ë…„'
    ]
    words = [word for word in words if word not in korean_stopwords and len(word) > 1]
    return ' '.join(words)

def convert_upload_date_to_kst(iso_datetime_str):
    try:
        dt_with_tz = parser.isoparse(iso_datetime_str)
        dt_kst = dt_with_tz.astimezone(timezone(timedelta(hours=9)))
        return dt_kst.strftime("%Y-%m-%d %H:%M:%S")
    except Exception as e:
        print("â›” uploadDate ì‹œê°„ íŒŒì‹± ì‹¤íŒ¨:", e)
        return ""

### **ë¹„ë™ê¸° í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤ ì •ì˜**

class YouTubeNewsCrawler:
    def __init__(self, channel_url, max_videos_to_collect=10, max_scrolls=10):
        self.channel_url = channel_url
        self.max_videos_to_collect = max_videos_to_collect
        self.max_scrolls = max_scrolls
        self.link_queue = asyncio.Queue()  # ë§í¬ ì •ë³´ë¥¼ ë‹´ì„ í
        self.result_queue = asyncio.Queue() # ìµœì¢… ê²°ê³¼ ë°ì´í„°ë¥¼ ë‹´ì„ í
        self.seen_titles = set()
        self.driver = None # Selenium WebDriver ì¸ìŠ¤í„´ìŠ¤

    async def _init_driver(self):
        """WebDriverë¥¼ ì´ˆê¸°í™”í•˜ëŠ” ë¹„ë™ê¸° í•¨ìˆ˜"""
        options = Options()
        options.add_argument("--headless")
        options.add_argument("--disable-gpu")
        options.add_argument("--lang=ko-KR")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--window-size=1920,1080")
        
        # Selenium WebDriver ì´ˆê¸°í™”ëŠ” ë™ê¸° í•¨ìˆ˜ì´ë¯€ë¡œ to_threadë¥¼ ì‚¬ìš©
        self.driver = await asyncio.to_thread(
            webdriver.Chrome, service=Service(ChromeDriverManager().install()), options=options
        )
        print("âœ… WebDriver ì´ˆê¸°í™” ì™„ë£Œ.")

    async def _close_driver(self):
        """WebDriverë¥¼ ì¢…ë£Œí•˜ëŠ” ë¹„ë™ê¸° í•¨ìˆ˜"""
        if self.driver:
            await asyncio.to_thread(self.driver.quit)
            print("âœ… WebDriver ì¢…ë£Œ.")
            self.driver = None

    async def link_collector(self):
        """
        ì±„ë„ í˜ì´ì§€ì—ì„œ ì˜ìƒ ë§í¬, ì œëª©, ì¸ë„¤ì¼ì„ ë¹„ë™ê¸°ì ìœ¼ë¡œ ìˆ˜ì§‘í•˜ì—¬ íì— ë„£ëŠ” ì½”ë£¨í‹´.
        Selenium ì‘ì—…ì€ to_threadë¥¼ í†µí•´ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰.
        """
        print(f"âœ… YouTube ì±„ë„ì— ì ‘ì† ì¤‘: {self.channel_url}")
        await asyncio.to_thread(self.driver.get, self.channel_url)
        
        try:
            await asyncio.to_thread(
                WebDriverWait(self.driver, 10).until,
                EC.presence_of_element_located((By.ID, "contents"))
            )
            print("âœ… ì±„ë„ ë¡œë”© ì™„ë£Œ.")
        except TimeoutException:
            print("â›” ì±„ë„ ë¡œë”© ì‹œê°„ ì´ˆê³¼.")
            return

        scroll_pause = 1.0
        scroll_count = 0
        last_height = await asyncio.to_thread(self.driver.execute_script, "return document.documentElement.scrollHeight")

        print("âœ… ì¡°ê±´ì— ë§ëŠ” ì˜ìƒ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
        while len(self.seen_titles) < self.max_videos_to_collect and scroll_count < self.max_scrolls:
            # ë™ê¸°ì ì¸ Selenium find_elements í˜¸ì¶œì„ to_threadë¡œ ê°ìŒˆ
            video_elements = await asyncio.to_thread(
                self.driver.find_elements, By.CSS_SELECTOR, "ytd-rich-item-renderer"
            )

            for element in video_elements:
                if len(self.seen_titles) >= self.max_videos_to_collect:
                    break
                
                try:
                    # Selenium ìš”ì†Œ ì†ì„± ì ‘ê·¼ ë° ì¶”ì¶œë„ to_threadë¡œ ê°ì‹¸ëŠ” ê²ƒì´ ì•ˆì „
                    badge_aria = await asyncio.to_thread(
                        lambda: element.find_element(By.CSS_SELECTOR, "badge-shape[aria-label]").get_attribute("aria-label")
                    )
                    match_min = re.search(r"(\d+)ë¶„", badge_aria)
                    match_sec = re.search(r"(\d+)ì´ˆ", badge_aria)
                    min_val = int(match_min.group(1)) if match_min else 0
                    sec_val = int(match_sec.group(1)) if match_sec else 0
                    total_seconds = min_val * 60 + sec_val

                    if total_seconds < 60 or total_seconds >= 300: # ì˜ìƒ ê¸¸ì´ ì¡°ê±´
                        continue

                    title_tag = await asyncio.to_thread(element.find_element, By.CSS_SELECTOR, "a#video-title-link")
                    title = await asyncio.to_thread(title_tag.get_attribute, "title")
                    href = await asyncio.to_thread(title_tag.get_attribute, "href")
                    link = f"[https://www.youtube.com](https://www.youtube.com){href}" if href.startswith("/watch") else href

                    thumbnail_anchor_tag = await asyncio.to_thread(element.find_element, By.CSS_SELECTOR, "a#thumbnail")
                    thumbnail_img_tag = await asyncio.to_thread(thumbnail_anchor_tag.find_element, By.CSS_SELECTOR, "yt-image > img")
                    thumbnail_url = await asyncio.to_thread(thumbnail_img_tag.get_attribute, "src")

                    if not thumbnail_url:
                        thumbnail_url = await asyncio.to_thread(thumbnail_img_tag.get_attribute, "data-src")
                    if not thumbnail_url:
                        video_id_match = re.search(r"v=([a-zA-Z0-9_-]{11})", link)
                        if video_id_match:
                            video_id = video_id_match.group(1)
                            thumbnail_url = f"[https://i.ytimg.com/vi/](https://i.ytimg.com/vi/){video_id}/maxresdefault.jpg"
                        else:
                            thumbnail_url = ""

                    if not title or title in self.seen_titles:
                        continue

                    self.seen_titles.add(title)
                    # ìˆ˜ì§‘ëœ ë§í¬ ì •ë³´ë¥¼ íì— ë¹„ë™ê¸°ì ìœ¼ë¡œ ë„£ìŒ
                    await self.link_queue.put({"title": title, "link": link, "thumbnail": thumbnail_url})
                    print(f"ğŸ”— ë§í¬ ìˆ˜ì§‘: '{title}' ({len(self.seen_titles)}ê°œ)")

                except NoSuchElementException:
                    continue # ìš”ì†Œê°€ ì—†ìœ¼ë©´ ê·¸ëƒ¥ ë„˜ì–´ê°
                except Exception as e:
                    print(f"âš ï¸ ë§í¬ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    continue

            # ìŠ¤í¬ë¡¤ ë™ì‘ë„ to_threadë¡œ ê°ìŒˆ
            await asyncio.to_thread(self.driver.execute_script, "window.scrollTo(0, document.documentElement.scrollHeight);")
            await asyncio.sleep(scroll_pause) # ë¹„ë™ê¸°ì ìœ¼ë¡œ ëŒ€ê¸°
            
            new_height = await asyncio.to_thread(self.driver.execute_script, "return document.documentElement.scrollHeight")
            if new_height == last_height:
                break
            last_height = new_height
            scroll_count += 1
        
        print(f"âœ… 1ì°¨ ì˜ìƒ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ. ì´ {len(self.seen_titles)}ê°œ.")
        # ë§í¬ ìˆ˜ì§‘ì´ ì™„ë£Œë˜ì—ˆìŒì„ ì•Œë¦¬ëŠ” ì‹ í˜¸ (ì˜µì…˜)
        for _ in range(5): # ì—¬ëŸ¬ detail_scraper ì½”ë£¨í‹´ì´ ì¢…ë£Œë  ìˆ˜ ìˆë„ë¡ ë§ˆì»¤ë¥¼ ì—¬ëŸ¬ ê°œ ë„£ìŒ
            await self.link_queue.put(None) 


    async def detail_scraper(self, worker_id):
        """
        íì—ì„œ ë§í¬ë¥¼ ê°€ì ¸ì™€ ìƒì„¸ í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•˜ê³  ì „ì²˜ë¦¬í•˜ëŠ” ì½”ë£¨í‹´.
        aiohttp + BeautifulSoup ë˜ëŠ” Selenium ì¤‘ ì„ íƒí•˜ì—¬ ì‚¬ìš© ê°€ëŠ¥.
        """
        while True:
            # íì—ì„œ ë¹„ë™ê¸°ì ìœ¼ë¡œ ë§í¬ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            item = await self.link_queue.get()
            if item is None: # ì¢…ë£Œ ì‹ í˜¸ë¥¼ ë°›ìœ¼ë©´ ì‘ì—… ì¢…ë£Œ
                self.link_queue.task_done()
                print(f"Worker {worker_id}: ë§í¬ í ì¢…ë£Œ ì‹ í˜¸ ìˆ˜ì‹ , ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break

            print(f"ğŸ” [Worker {worker_id}] '{item['title']}' ì˜ìƒ ìƒì„¸ ì •ë³´ ì¶”ì¶œ ë° ì „ì²˜ë¦¬ ì¤‘...")
            
            extracted_prompt = ""
            upload_time = ""

            try:
                # ë°©ë²• 1: aiohttp + BeautifulSoup ì‚¬ìš© (ì¶”ì²œ: ë¹ ë¥´ê³  ë¦¬ì†ŒìŠ¤ ì†Œëª¨ ì ìŒ)
                # ì´ ë°©ë²•ì€ JavaScript ë Œë”ë§ì´ í•„ìš” ì—†ëŠ” ê²½ìš°ì— ë§¤ìš° íš¨ìœ¨ì 
                async with aiohttp.ClientSession() as session:
                    async with session.get(item["link"]) as response:
                        response.raise_for_status() # HTTP ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ˆì™¸ ë°œìƒ
                        html = await response.text()
                        soup = BeautifulSoup(html, 'html.parser')

                        # uploadDate ì¶”ì¶œ
                        json_script = soup.find('script', {'type': 'application/ld+json'})
                        if json_script:
                            try:
                                json_data = json.loads(json_script.string)
                                upload_datetime_str = json_data.get("uploadDate", "")
                                upload_time = convert_upload_date_to_kst(upload_datetime_str)
                            except Exception as e:
                                print(f"â›” [Worker {worker_id}] uploadDate íŒŒì‹± ì‹¤íŒ¨: {e}")
                        
                        # prompt ì¶”ì¶œ (ìŠ¤í¬ë¦½íŠ¸ íƒœê·¸ ë‚´ description ì°¾ê¸° ë˜ëŠ” ì§ì ‘ DOM íƒìƒ‰)
                        # ìœ íŠœë¸ŒëŠ” ë™ì ìœ¼ë¡œ ì½˜í…ì¸ ë¥¼ ë¡œë“œí•˜ë¯€ë¡œ, ì´ ë°©ë²•ë§Œìœ¼ë¡œëŠ” í”„ë¡¬í”„íŠ¸ ì¶”ì¶œì´ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                        # Seleniumì´ í•„ìš”í•˜ë‹¤ë©´ ì•„ë˜ ë°©ë²• 2ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
                        # ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œë¡œ json_dataì—ì„œ descriptionì„ ì°¾ê±°ë‚˜, ì¼ë°˜ì ì¸ HTML ìš”ì†Œì—ì„œ ì°¾ìœ¼ë ¤ ì‹œë„
                        # NOTE: YouTubeì˜ descriptionì€ JSë¡œ ë¡œë“œë˜ëŠ” ê²½ìš°ê°€ ë§ì•„ ì•„ë˜ ì½”ë“œë¡œ ì•ˆë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                        # ì´ ê²½ìš° Seleniumì„ ê³„ì† ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
                        # extracted_prompt = soup.find('meta', {'name': 'description'})
                        # if extracted_prompt:
                        #     extracted_prompt = extracted_prompt.get('content')
                        # else: # Fallback to Selenium if direct fetch fails for prompt
                        #     extracted_prompt = await self._get_prompt_with_selenium(item['link'])
                        
                        # ìœ íŠœë¸Œì˜ descriptionì€ ë³´í†µ íŠ¹ì • ìŠ¤í¬ë¦½íŠ¸ íƒœê·¸ ë‚´ì— ìˆê±°ë‚˜, ë™ì ìœ¼ë¡œ ë¡œë“œë©ë‹ˆë‹¤.
                        # ë”°ë¼ì„œ BeautifulSoupë§Œìœ¼ë¡œëŠ” ì¶”ì¶œì´ ì–´ë ¤ìš¸ ìˆ˜ ìˆì–´, Seleniumì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” í™•ì‹¤í•©ë‹ˆë‹¤.

                        # ë”°ë¼ì„œ, **í”„ë¡¬í”„íŠ¸ ì¶”ì¶œì€ Seleniumì— ì˜ì¡´í•˜ëŠ” ê²ƒì´ í˜„ì¬ë¡œì„œëŠ” ê°€ì¥ í™•ì‹¤í•©ë‹ˆë‹¤.**
                        # ë§Œì•½ aiohttp+BeautifulSoupìœ¼ë¡œ ëŒ€ë¶€ë¶„ì˜ ì •ë³´ë¥¼ ì–»ê³ , í”„ë¡¬í”„íŠ¸ë§Œ Seleniumìœ¼ë¡œ ì–»ê³  ì‹¶ë‹¤ë©´
                        # ì•„ë˜ì™€ ê°™ì´ _get_prompt_with_seleniumì„ í˜¸ì¶œí•˜ë„ë¡ ì„¤ê³„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                        extracted_prompt = await self._get_prompt_with_selenium(item['link'])


            except aiohttp.ClientError as e:
                print(f"âŒ [Worker {worker_id}] HTTP ìš”ì²­ ì‹¤íŒ¨ ('{item['title']}'): {e}")
                self.link_queue.task_done()
                continue
            except Exception as e:
                print(f"âŒ [Worker {worker_id}] ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì¤‘ ì¼ë°˜ ì˜¤ë¥˜ ë°œìƒ ('{item['title']}'): {e}")
                self.link_queue.task_done()
                continue

            if not extracted_prompt:
                print(f"âš ï¸ [Worker {worker_id}] '{item['title']}' ({item['link']}) ì˜ìƒì—ì„œ í”„ë¡¬í”„íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì´ ì˜ìƒì€ ê±´ë„ˆëœœë‹ˆë‹¤.")
                self.link_queue.task_done()
                continue
            
            # ì „ì²˜ë¦¬ëœ ì œëª©ê³¼ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
            cleaned_title_val = clean_title(item["title"])
            cleaned_prompt_val = clean_text_for_vectorization(extracted_prompt)

            # ìµœì¢… ê²°ê³¼ íì— ë°ì´í„° ë„£ê¸°
            await self.result_queue.put({
                "title": item["title"],
                "prompt": extracted_prompt,
                "link": item["link"],
                "upload_time": upload_time,
                "thumbnail_link": item.get("thumbnail", ""),
                "cleaned_title": cleaned_title_val,
                "cleaned_prompt": cleaned_prompt_val
            })
            self.link_queue.task_done() # íì—ì„œ í•˜ë‚˜ì˜ ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŒì„ ì•Œë¦¼

    async def _get_prompt_with_selenium(self, video_link):
        try:
            await asyncio.to_thread(self.driver.get, video_link)
            # await asyncio.sleep(2) # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            
            # 'ë”ë³´ê¸°' ë²„íŠ¼ í´ë¦­ ì‹œë„ (ë¹„ë™ê¸°ì ìœ¼ë¡œ)
            try:
                expand_btn = await asyncio.to_thread(
                    WebDriverWait(self.driver, 5).until, # 5ì´ˆ ëŒ€ê¸°
                    EC.element_to_be_clickable((By.XPATH, '//*[@id="expand"]'))
                )
                await asyncio.to_thread(expand_btn.click)
                await asyncio.sleep(1) # í´ë¦­ í›„ ë‚´ìš© ë¡œë”© ëŒ€ê¸°
            except TimeoutException:
                pass # 'ë”ë³´ê¸°' ë²„íŠ¼ì´ ì—†ê±°ë‚˜ í´ë¦­í•  ìˆ˜ ì—†ìœ¼ë©´ ë„˜ì–´ê°
            except NoSuchElementException:
                pass # 'ë”ë³´ê¸°' ë²„íŠ¼ ìì²´ê°€ ì—†ìœ¼ë©´ ë„˜ì–´ê°

            # í”„ë¡¬í”„íŠ¸ ì¶”ì¶œ (ë¹„ë™ê¸°ì ìœ¼ë¡œ)
            try:
                prompt_element = await asyncio.to_thread(
                    WebDriverWait(self.driver, 5).until,
                    EC.presence_of_element_located((By.XPATH, '//*[@id="description-inline-expander"]/yt-attributed-string/span/span[1]'))
                )
                return await asyncio.to_thread(lambda: prompt_element.text.strip())
            except TimeoutException:
                print(f"âš ï¸ '{video_link}' ì˜ìƒì—ì„œ í”„ë¡¬í”„íŠ¸ ìš”ì†Œ ì°¾ê¸° ì‹œê°„ ì´ˆê³¼.")
                return ""
            except NoSuchElementException:
                print(f"âš ï¸ '{video_link}' ì˜ìƒì—ì„œ í”„ë¡¬í”„íŠ¸ ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return ""

        except Exception as e:
            print(f"âŒ Selenium í”„ë¡¬í”„íŠ¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ('{video_link}'): {e}")
            return ""

    async def run_crawler(self):
        """í¬ë¡¤ë§ ì‘ì—…ì„ ì´ê´„í•˜ëŠ” ë©”ì¸ ì½”ë£¨í‹´"""
        await self._init_driver() # WebDriver ì´ˆê¸°í™”
        
        start_time = time.time()
        
        # ë§í¬ ìˆ˜ì§‘ê¸°ì™€ ìƒì„¸ ì •ë³´ ì¶”ì¶œê¸° ì½”ë£¨í‹´ ìƒì„±
        collector_task = asyncio.create_task(self.link_collector())
        
        # ì—¬ëŸ¬ ê°œì˜ detail_scraper workerë¥¼ ë™ì‹œì— ì‹¤í–‰ (ì˜ˆ: 3ê°œ)
        num_workers = 3 
        scraper_tasks = [
            asyncio.create_task(self.detail_scraper(i + 1)) for i in range(num_workers)
        ]

        # ëª¨ë“  ë§í¬ê°€ ìˆ˜ì§‘ë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¼
        await collector_task
        
        # ë§í¬ íì— ëª¨ë“  ì‘ì—…ì´ ì²˜ë¦¬ë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¼ (detail_scraperë“¤ì´ ëª¨ë‘ ì‘ì—… ë§ˆì¹  ë•Œê¹Œì§€)
        await self.link_queue.join() 

        # ëª¨ë“  detail_scraper worker ì¢…ë£Œ ì‹ í˜¸ ë³´ë‚´ê¸° (None ë§ˆì»¤ê°€ ì²˜ë¦¬ë  ë•Œê¹Œì§€ ëŒ€ê¸°)
        # ì´ë¯¸ link_collectorì—ì„œ Noneì„ ì¶©ë¶„íˆ ë„£ì—ˆìœ¼ë¯€ë¡œ, ì´ ë¶€ë¶„ì€ ë¶ˆí•„ìš”í•  ìˆ˜ ìˆìŒ.
        # í•˜ì§€ë§Œ ëª…ì‹œì ìœ¼ë¡œ ëª¨ë“  workerê°€ ì¢…ë£Œë˜ë„ë¡ ë³´ì¥í•˜ëŠ” ê²ƒì´ ë” ì•ˆì „.
        for task in scraper_tasks:
            task.cancel() # ë” ì´ìƒ ì²˜ë¦¬í•  ë§í¬ê°€ ì—†ìœ¼ë¯€ë¡œ worker taskë¥¼ ì·¨ì†Œ

        # ê²°ê³¼ íì—ì„œ ëª¨ë“  ë°ì´í„°ë¥¼ ìˆ˜ì§‘
        all_crawled_data = []
        while not self.result_queue.empty():
            all_crawled_data.append(await self.result_queue.get())

        # ID ë¶€ì—¬ ë° ì •ë ¬ (ì˜µì…˜: publishedAt ê¸°ì¤€ ì •ë ¬ í›„ ID ë¶€ì—¬)
        # YouTubeëŠ” ìµœì‹ ìˆœìœ¼ë¡œ ì •ë ¬ë˜ì–´ ë³´ì—¬ì§€ë¯€ë¡œ, ìˆ˜ì§‘ëœ ìˆœì„œëŒ€ë¡œ IDë¥¼ ë¶€ì—¬í•´ë„ ë¬´ë°©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        # ë§Œì•½ publishedAt ê¸°ì¤€ìœ¼ë¡œ ì •í™•íˆ ìµœì‹ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
        # all_crawled_data.sort(key=lambda x: x['upload_time'], reverse=True) 
        
        final_results = []
        for i, data in enumerate(all_crawled_data):
            data['id'] = i + 1 # 1ë¶€í„° ì‹œì‘í•˜ëŠ” ID ë¶€ì—¬
            final_results.append(data)

        df = pd.DataFrame(final_results)

        if not df.empty:
            df_to_save = df[["id", "title", "prompt", "link", "cleaned_title", "cleaned_prompt", "thumbnail_link", "upload_time"]]
            # df_to_save.to_csv("original_data.csv", index=False, encoding="utf-8-sig")
            # print("âœ… íŒŒì¼ ì €ì¥ ì™„ë£Œ: original_data.csv")
            
            # JSON íŒŒì¼ ì €ì¥ ì¶”ê°€
            json_file_path = "ìµœì í™”_sample_data.json"
            with open(json_file_path, 'w', encoding='utf-8') as f:
                json.dump(df_to_save.to_dict('records'), f, ensure_ascii=False, indent=4)
            print(f"âœ… JSON íŒŒì¼ ì €ì¥ ì™„ë£Œ: {json_file_path}")

        else:
            print("â„¹ï¸ ìˆ˜ì§‘ëœ ì˜ìƒ ì •ë³´ê°€ ì—†ì–´ CSV/JSON íŒŒì¼ì„ ìƒì„±í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
        await self._close_driver() # WebDriver ì¢…ë£Œ

        end_time = time.time()
        total_time = end_time - start_time
        print(f"\nì´ ì†Œìš”ì‹œê°„ : {total_time:.2f} ì´ˆ")

        return df

async def main():
    crawler = YouTubeNewsCrawler(channel_url="https://www.youtube.com/@newskbs/videos", max_videos_to_collect=10, max_scrolls=5) # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ìˆ˜ì§‘ ê°œìˆ˜ ë° ìŠ¤í¬ë¡¤ íšŸìˆ˜ ì¤„ì„
    crawled_data_df = await crawler.run_crawler()
    if not crawled_data_df.empty:
        print("\n--- í¬ë¡¤ë§ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸° (ìƒìœ„ 5ê°œ) ---")
        print(crawled_data_df.head())
    else:
        print("\n--- í¬ë¡¤ë§ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ---")

if __name__ == "__main__":
    asyncio.run(main())